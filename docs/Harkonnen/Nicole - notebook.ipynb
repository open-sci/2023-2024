{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First week - getting to know each other\n",
    "\n",
    "### **To sum up**\n",
    "- Reviewed lecture materials and notes.\n",
    "- Focused on the research question with initial steps of exploration: considered Crossref and documentation related to the API (initial attempts at API requests); conducted a brief search on the OpenCitation data model and the hypothetical properties useful for our combined research.\n",
    "- Met with other team members to discuss the abstract.\n",
    "\n",
    "### Small steps\n",
    "- The entire **Crossref** database is a 185GB file. Among all this data, 491602 is the total number of [peer-reviewed](https://api.crossref.org/types/peer-review/works?rows=0) records stored. In Crossref, there are three different versions of the API - for us, both the Public and the Polite versions are suitable (the Plus version is not useful). While the former is completely anonymous but often inconsistent, the latter is generally more reliable. The API is delicate and can easily go down. There is a documentation page created specifically to list useful tips for our future work (e.g., query bibliographic parameters, rows, etc.): this is to avoid slow queries or false positives. Pay attention to persistent errors as they can interrupt the query reception by the system. The document also includes some Python libraries that could be useful in case of long queries.\n",
    "- The **OpenCitation data model** lacks direct links between a cited article and the citing peer-review. The property that may be most suitable for our work is the object property \"*is document context for*\" (\"A property relating a document to the role for which that document provides the context (e.g. relating a document to the role of author or peer-reviewer of that document)\"). The roles to consider for the agents in this case could be \"*reviewer*\" or \"*peer reviewer*\".\n",
    "\n",
    "### What will follow? Getting started. \n",
    "- Clarify which API call to use to precisely retrieve the review and the reference article in Crossref.\n",
    "- Clarify the usage of the OC ontology and understand how many articles have at least one review.\n",
    "- Determine how to transform the JSON from Crossref into RDF triples of OC (starting from a single review as well). \n",
    "\n",
    "> ### *Usefull links*\n",
    ">- [Crossref API documentation ](https://https://www.crossref.org/documentation/retrieve-metadata/rest-api/)\n",
    ">- [OC ontology](https://opencitations.github.io/ontology/current/ontology.html#d4e240)\n",
    ">    - [PRO, the Publishing Roles Ontology](https://sparontologies.github.io/pro/current/pro.html#d4e825)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second and third week\n",
    "\n",
    "### **To sum up**\n",
    "- I have read the material and watched the lecture recordings.\n",
    "- I have a clearer idea of how to query Crossref for the information we need.\n",
    "- Drafted an initial version of the Data Management Plan (DMP) with the team.\n",
    "- Developed an initial idea of the workflow.\n",
    "\n",
    "### New clues:\n",
    "- Reading the Crossref API documentation page, it seems that querying the database requires the use of the \"type\" filter. In Crossref, resources are also registered based on the reference category (monograph, report, book-section, etc. – the complete list of types can be obtained through the query https://api.crossref.org/types?rows=30&mailto= and insert your own email address at the end of the string).\n",
    "   - The query we need is https://api.crossref.org/works?filter=type:peer-review:\n",
    "     - This also seems to be confirmed by the fact that the query calculates that the number of results is equal to those counted by last week's query (which calculated the total number of peer reviews) – for confirmation, rerun both queries because the number of peer reviews grows over time.\n",
    "      - The number of visible results is limited (around a hundred). Instead of making millions of queries, we need to use a Python library. Crossref-commons-py was created by Crossref developers. The SAMPLING section of the library might be particularly useful for us.\n",
    "- With doubts and perplexities, we have compiled an initial version of the DMP for both the future output database and the software we want to create.\n",
    "- With the team, we have aligned on what we need to do to obtain research outputs:\n",
    "   - Retrieve data from Crossref with appropriate queries. Documents in Crossref have a link that explicitly connects the peer review with the resource receiving the review.\n",
    "   - Retrieve data from OpenCitation with appropriate queries. Documents are not linked together directly (in the way Crossref does). Instead, the author of the review must also be considered: referring to last week's notes, the \"is document context for\" property might be useful.\n",
    "   - Compare the two datasets and ignore Crossref reviews that already have a counterpart in Open Citation.\n",
    "   - Using the Open Citation model, convert the remaining data into RDF triples faithful to the model.\n",
    "\n",
    "### What will follow?:\n",
    "\t\n",
    "- Clarifying how to use the Python library for Crossref.\n",
    "- Verifying that the DMP is of good quality.\n",
    "- Clarifying how to extract data from OpenCitation.\n",
    "\t\n",
    "> ### *Usefull links*\n",
    ">- [Crossref Commons for Python](https://gitlab.com/crossref/crossref_commons_py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weeks Four, Five, and Six\n",
    "\n",
    "### **To Sum Up**\n",
    "- I delved deeper into the Crossref documentation and clarified the relationships between entities.\n",
    "- We downloaded the Crossref chunks.\n",
    "- I contributed to the development of the extraction and transformation phases of Crossref entities.\n",
    "- I assisted in drafting the protocol.\n",
    "- I read and commented on the DMP (Data Management Plan) of the Atreides group. We received significant feedback on our DMP and protocol drafts.\n",
    "\n",
    "### **New Clues**\n",
    "- To ensure our project results comply with the FAIR principles, it is necessary to use the annual dump of the database instead of the API. Nonetheless, the API is a useful tool for quickly resolving certain doubts.\n",
    "- The OpenCitations data structure is evolving: it is based on relationships between unique entities, which in the near future will follow an OMID-to-OMID model. Currently, the identifier is generated following specific guidelines and a lookup map, which can be found in the project's public repository.\n",
    "\n",
    "> ### **Useful Links**\n",
    "> - **Crossref Preprint Relations**  \n",
    ">   - [Discovering relationships between preprints and journal articles](https://www.crossref.org/blog/discovering-relationships-between-preprints-and-journal-articles/)  \n",
    ">   - [Better preprint metadata through community participation](https://www.crossref.org/blog/better-preprint-metadata-through-community-participation/)  \n",
    ">   - [Leaving the house: Where preprints go](https://www.crossref.org/blog/leaving-the-house-where-preprints-go/)  \n",
    ">   - [Preprints’ growth rate ten times higher than journal articles](https://www.crossref.org/blog/preprints-growth-rate-ten-times-higher-than-journal-articles/)  \n",
    "> - **Crossref Peer Reviews**  \n",
    ">   - [Making peer reviews citable, discoverable, and creditable](https://www.crossref.org/blog/making-peer-reviews-citable-discoverable-and-creditable/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weeks Seven and Eight\n",
    "\n",
    "### **To Sum Up**\n",
    "- I contributed to the research and writing of the paper.  \n",
    "- We prepared the presentation.  \n",
    "- We revised the DMP (Data Management Plan) and the protocol.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week Eight\n",
    "\n",
    "### **To Sum Up**\n",
    "- We revised all parts of our project based on the feedback received after the presentation and the professor's corrections. The most significant updates concerned the extraction of authors from peer reviews and the relationship between anonymous and signed reviews.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
