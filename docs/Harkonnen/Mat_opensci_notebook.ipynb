{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Science's Notebook 23/24\n",
    "\n",
    "---------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question:\n",
    "_Is that possible to create a new index of citations which contains typed citations where a peer review (citing entity) reviews (specific citation function) a publication (cited entity)? What is the necessary transformation of the Crossref dump necessary to create such an index to be compliant with the OpenCitations Data Model? What are the top publication venues in terms of the number of peer reviews received? How many peer reviews in Crossref are included in OpenCitations Meta? How many articles that have been reviewed by a peer review are included in OpenCitations Meta?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matteo Guenci - 0001102177 - Harkonnen\n",
    "## Week 1 | 21/03 -> 27/03\n",
    "### Day 1 | 26/03/2024 | First entry\n",
    "\n",
    "<center><img src=\"https://i.kym-cdn.com/photos/images/original/002/235/045/934.jpg\"></center>\n",
    "\n",
    "- Notebook has been created and uploaded on the [23/24 Open Science repository][]\n",
    "- The research question has been approached\n",
    "- A call has been made with the other members of the group to create an abstract of the project\n",
    "\n",
    "The following is a consideration that must be taken in account: to this day, given the fact that i just started the open science course i'm not really sure about what i'm writing below, in the sense that it may be possible that in the future by looking again at the following thoughts i will be horrified/amused by the things that i wrote, that being said these are my consideration up untill now. \n",
    "- To better understand how to resolve the reaearch question i looked at the definition given in the first lecture that we had about Crossref and OpenCitation Meta:\n",
    "  - Being that \"Each DOI registered in the Crossref system is associated with a URL to the publication’s webpage and accompanied with the metadata of the publications\" and \"Crossref provides a REST API to retrieve data about the entities and provides annual dumps\" i would imagine that is possible to create an index such as the one requested by the research question, maybe by taking a look at the annual dumps.\n",
    "  - Then i asked myself if there was any place in which i could find somethin even better instead of looking at the annual dumps of Crossref, that's where [COCI][] an \"RDF dataset containing details of all the citations that are specified by the open references to DOI-identified works present in Crossref\" stepped in. So it should be theoretically possible to check using its endpoint if the triple requested by the research question does exist\n",
    "  - By doing a quick research i found out that Crossref's annual dumps are in JSON, which i'm pretty sure that is not compliant with the OCDM, in order to be compliant it is necessary to transform the annual dumps.\n",
    "  - As for the fina parts of the research question further analysis are required and to this moment i'm unable to produce any consideration.\n",
    "\n",
    "\n",
    "[23/24 Open Science repository]: https://github.com/open-sci/2023-2024       \"Open Science\"\n",
    "[COCI]: https://opencitations.net/index/coci   \"COCI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 and 3 | 28/03 -> 3/04 and 04/04 -> 10/04\n",
    "### Day 1 | 4/04/2024\n",
    "- Had a call with the other members of the group to organize our work. Beside the organization we didn't do that much but that's ok. A good work follows a good organization. Being the Easter week and being that everyone was pretty much back at home we did the most we could do:\n",
    "    - We talked about the first draft of the DMP.\n",
    "    - We talked about how to proceed to get the information we need to create the new index requested by the research qurestion.\n",
    "\n",
    "### Day 2 | 6/04/2024\n",
    "- We met to get some work. We created, not without any perplexity, the two DMPs we needed. Some fields that we had to fill during this process confused us, but i guess it is normal for the moment. We decided to meet again soon to work on the protocol that we have to develop. More than that:\n",
    "    - I took a look at the crossref ocumentation, the one on [github] and the one on their [website]  for the rest api of crossref, thanks to that i have a clearer idea on how to get some data we will definitely need.\n",
    "    - While we were talking we came up with a solution(?) that can potentially be effective and that we also discussed the week before but now we are getting more confident with it. Basically it is a workaround: while on Crossref it is quite easy to get data about documents and their peer review (because in Crossref there is a direct link that connects the peer review with the resource receiving the review), the OpenCitation data model lacks direct links between cited articles and their respective peer reviews so we thought that an important property for our purpose could be the \"[is document context for]\" a property \"relating a document to the role for which that document provides the context (e.g. relating a document to the role of author or peer-reviewer of that document). In this case, roles like \"reviewer\" or \"peer reviewer\" are pertinent considerations. In simpler words: find the document for which in that moment the author covered the role of reviewer or peer reviewer and then allineate them to the results taken from crossref\n",
    "    - Thanks to my group i learned important informations such as the fact that we won't need to query an infinite amount of time crossref or find another workaround that maybe is in the documentation because we'll just need to use the appropriate [library].\n",
    "\n",
    "\n",
    "### Doubts\n",
    "    \n",
    "<center><img src=\"https://images.fineartamerica.com/images/artworkimages/medium/3/dont-panic-hitchhikers-guide-to-the-galaxy-amin-sholeh-transparent.png\" heigt=200px width=400px></center>\n",
    "\n",
    "- While this seems a good plan to me (even if sometimes i stiil get stuck while trying to follow our own workaround), i have some perplexities regarding how we will pragmatically allineate all those results, how we will transform the format of results and all these kind of questions. I guess that when we'll start to develop our software everything will be clearer,hopefully, since the software itself is still one of my perplexities. But i trust myself and my groups, Furthermore, reassuring words echoing in my head, which I once read in a fairly famous book, also comfort me: \"Don't Panic. It's the first helpful or intelligible thing anybody's said to me all day.” so i won't. \n",
    "- We asked ourselves if there could be a problem regarding the fact that maybe while we're working the number of the reviews will grow.\n",
    "- I personally have a doubt regarding the fact that in this early stage of the work creating a DMP and a protocol without all the informations that we will have at the end of the project may be an iterative work that will require us many adjustments. I trust the fact that during the first lectures of this course Professor Peroni told us that it is the best choice to get these things done in advance\n",
    "\n",
    "\n",
    "### Future developments\n",
    "- As said before we will meet again to work on the protocol.\n",
    "- I have to get a deeper look at all the documentations needed to start developing the software.\n",
    "- After indulging in a binge of reading, it might be a good idea to adopt the same approach used for DMPs and the protocol, meaning starting work early so it doesn't become overwhelming later on.\n",
    "\n",
    "\n",
    "\n",
    "[github]: https://github.com/CrossRef/rest-api-doc#resource-components       \"Crossref\"\n",
    "[website]: https://www.crossref.org/documentation/retrieve-metadata/rest-api/        \"Crossref\"\n",
    "[is document context for]: https://sparontologies.github.io/pro/current/pro.html#d4e124    \"PRO ontology\"\n",
    "[library]: https://gitlab.com/crossref/crossref_commons_py    \"crossref_common_py\"\n",
    "[Daniele]: https://github.com/SpedicatiDaniele    \"Daniele\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 2 | 8/04/2024\n",
    "We met once again to define how the protocol should look like. During our meeting we were able to define a proper (hopefully) workflow to implement that should cover our task that is more or less looking like this:\n",
    "- Take all the peer reviews on Crossref, keeping track of the venues in which they're in\n",
    "- Transform the data in order to let them be compliant with the OC data model (so transform them into RDF)\n",
    "- Find the doi of the peer reviews present in OC and then create two lists:\n",
    "    - one for peer reviews that do not have any correspondance in OC, whether they are not in OC or they're not registered as peer reviews\n",
    "    - one for peer reviews that are already in OC\n",
    "- Isolate venues of the first lists and find out which venue has the highest number of peer reviews\n",
    "- Count how many DOIs are in the second list\n",
    "- Count how many DOIsof articles are in the second list\n",
    "\n",
    "More or less this should be the workflow we intend to implement to answer all the questions in our research question.\n",
    "\n",
    "\n",
    "### Doubts\n",
    "\n",
    "During our meeting we were a little bit confuse by our research question, because we didn't properly understand whether we should use our newly created index tpo answer all the sub-questions or if we should use already existing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4 | *11/04 -> 17/04*\n",
    "### Day 1 |16/04/2024\n",
    "\n",
    "<center><img src=\"https://github.com/matteo-guenci/IMD-materials/blob/main/foto_unrelated/harkonnen_reviewing.jpg\"></center>\n",
    "\n",
    "Today i worked on the review about the protocol produced by team Atreides. I have to say that their protocol seems to be weel thought and conceived, of course being it in an early stage it's not possible to say where their research is going for now. I tried to focus on constructive suggestions mostly regarding the clarity about their exposition in the manipulation and in the interpretation of the results because honestly that is pretty much all that i have to say about it. All the parts about how to implement their software seem to be quite solid for now, the chain of query-data manipulation seem to be effective in producing answer they may need.\n",
    "Also confronting with the other team we found out that pasting the doi of the DMPs on Qeios lead to nowhere, this may be something to ask for further clarifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5 | *18/04 -> 25/04*\n",
    "Me and the rest of the gorup saw eachother to actually define a plan to develop the software. Using our dear old pen and paper we spent some days trying to crack the best possilble strategy to actually retrieve and use whatever we needed. As time get by is is kind of frustrating see that every day studying a little bit more reveal to us that the accomplishment we thought we had achieved the days before are not that good. But it is what it is. \n",
    "We are starting to think that we will need to adjust the description of the protocol deeply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 6 | *26/04 -> 2/05*\n",
    "As i stated in other entries of this diary getting to work is kind of strange beacuse one day you do something that looks like progress and the day after it is probably useless. A huge problem we have is the size of the Crossref dump we need to have for our research. It is prohibitive for our machines that will probably have problems processing it during all the phases of this project. We asked for help about it\n",
    "In the meantime we started using Colab to see if we could run some code efficiently one some sample of data retrieved from the Crossref API. We managed to devise a first version of the class PeerReview that we will likely use in the end of the project when we'll need to populate our data with the right characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7 | *3/05 -> 9/05*\n",
    "Between classes, exams and personal matters, we didn't had the chance to work a lot this week. We still had some work done regarding the pre-processing phase. We managed to have our Crossref dump to be divided in 18 zipped parts, it is easier to deal with. Even if the download of each single part required more or less 45 minutes. A first try to work on the files without extracting them made us realize that time is not in our favor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8 | *10/05 -> 17/05*\n",
    "### Day 3 | 13/05/2024\n",
    "\n",
    "Much of this week has been devoted to understanding what and how to efficiently extract from the individual chunks of the crossref dump. We made many attempts, most of which resulted in failure due to the very large amount of data and the computational capabilities of our computers. As a result of these repeated ‘failures’, we had to devise an efficient way to handle this enormous amount of data, and considering that our extraction tests often took us at least 45 minutes per file, much of that time was spent staring at the computer grinding out data. I must say that all this gave me a bit of a sense of helplessness, but then again, the data pre-processing phase is always like that.\n",
    "\n",
    "\n",
    "### Day 4 | 14/05/2024\n",
    "On my own, I tried alternative solutions, one of which was to use Spark. Initially (and unsuccessfully) I tried setting everything up on my main pc, using this Windows as my operating system, which took me almost half a day that I could have used more productively in hindsight. The same evening, on a secondary PC I have at home and which uses Linux as its operating system, I continued this experiment which promises. \n",
    "\n",
    "\n",
    "### Day 5 | 15/05/2024\n",
    "\n",
    "Some problems derived from the approach described above made me give up. Firstly, I felt that being the only one who could use Spark efficiently in my group would create more problems than anything else. Secondly, my secondary PC is not powerful enough to replace the collective effort of 4 group members. Thirdly, studying a framework like spark would have taken time to learn how to use it decently and how to explain it above all. Basically I wasted some time but during the evaluation phase of this approach it might have seemed like a good idea so it was worth a try especially because I knew I could count on the efforts of the other members of my group who in the meantime continued to work on solving the problems described above in a more \"canonical\" way.\n",
    "\n",
    "### Day 6 | 16/05/2024\n",
    "\n",
    "We are starting to think that we will probably need to see eachoter every day untill the presentation. We are not getting quite good results for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 9/10 | *18/05 -> 27/05*\n",
    "I will write a single entry for this workweek since we saw each other and worked incessantly every day. Finally, we achieved promising results through the use of more suitable data structures and especially with the help of Python libraries that maximize the computational power of each machine. Another solution was to implement a work logic that involved reading, analyzing, and transcribing the necessary information in small batches. On average, each chunk took about an hour to extract data regarding peer-reviewed and non-peer-reviewed items. To work more efficiently, we divided the tasks into thematic sections. I mainly focused on data processing, which includes extraction, cleaning, and organization. Fortunately, for the peer-reviewed data, we were able to use ready-made codes provided by OpenCitations (COCI). An honorable mention is necessary for Ivan Heibi, to whom we asked several questions to better manage some sections of our project.\n",
    "\n",
    "I spent a couple of days feeling nervous because, during the join phase of the dataframes concerning peer-reviewed and non-peer-reviewed items, I obtained results that were completely different from what we expected. These two days dedicated to solving memory issues caused by handling 36 large dataframes and resolving unexpected results were not very productive. I believe I entered a state of grace at sunset this week because, on my way home, I would think of a solution that I could implement after some effort. I must say that without external help (such as the vast variety of internet resources and suggestions from more experienced people), I would have needed more time to solve these problems.\n",
    "\n",
    "In essence, the memory issues were resolved thanks to Polars, which allows dataframes to be read without loading them into memory until operations are applied, and it generally performs better with large amounts of data (thank you for existing, scan_csv and sink_csv). The issue with the unexpected results was due to my complete oversight of a warning given at the start of this project: the normalization of DOIs. Once normalized, the match accuracy approached 100% (only 10 DOIs were not found out of nearly half a million).\n",
    "\n",
    "The next challenge was converting all these software products into programs callable from terminals, where the logic of class-based programming was present. Dusting off the knowledge acquired during the Data Science course, we managed to solve this with somewhat mixed success. A significant amount of time was dedicated to creating the run.py file, which was supposed to gather everything we had produced into a single file. The creation of this software was completed just in time because shortly before the deadline, we realized it wasn't working. This problem arose because all the issues caused by data processing made us neglect other tasks of this project such as reviewing the DMP, reviewing the protocol, writing the article, creating the slides, and creating visualizations of the results. All tasks that we eventually managed to successfully resolve but that clearly required a lot of effort.\n",
    "\n",
    "I would like to thank my colleagues who had to endure my obsessions at times, which did not always yield results. Without them, I wouldn't have been able to solve several problems and, most importantly, complete this project, which required a 360-degree effort. Overall, we are satisfied with the results we have achieved. We are aware that this project can probably be improved, which we will need to do anyway, but we can still say we are happy with the results obtained because we believe that the creation of PROCI (our Peer Review OpenCitations Index) can be a valuable contribution. This awareness makes us happy and gives us a sense of fulfillment, knowing that we have created something potentially useful.\n",
    "\n",
    "Post Credit:\n",
    "I think the presentation went well; we are awaiting feedback from the professor. Now begins the study of the theoretical part for the second part of the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefull things to remember:\n",
    "- The term \"typed citation\" in this context could be translated as \"citation with defined type\". It refers to the ability to define specific types of citations within an index, such as a citation where a peer review (citing entity) reviews (specific citation function) a publication (cited entity). In this context, \"typed citation\" indicates that the citation is characterized by a specific relationship between the source and the target of the citation, which is defined based on a specific type of interaction between the two involved entities.\n",
    "\n",
    "- The [OpenCitations Meta database][] stores and delivers bibliographic metadata for all publications involved in the OpenCitations Index. For each publication, the metadata exposed by OpenCitations Meta includes the publication's title, type, venue (e.g. journal name), volume number, issue number, page numbers, publication date, and identifiers such as Digital Object Identifiers (DOIs) and PubMed Identifiers (PMIDs). Currently OpenCitations Meta contains more than 90 million bibliographic entities that can be accessed and queried through a REST API and downloaded as a [full dump][].\n",
    "\n",
    "- Crossref is a not-for-profit membership association which aims at promoting the development and cooperative use of new and innovative technologies to speed and facilitate scientific and other scholarly research. Crossref is one of the ten International DOI registration agencies, and allows its members to register the DOIs of their publications. Each DOI registered in the Crossref system is associated with a URL to the publication’s webpage and accompanied with the metadata of the publications. Crossref provides a REST API to retrieve data about the entities and provides annual dumps. The entities Crossref comprises include also peer reviews of other articles, as shown in the exemplar result of the of the following API call: \n",
    "\n",
    "- [COCI][], the OpenCitations Index of Crossref open DOI-to-DOI citations, is an RDF dataset containing details of all the citations that are specified by the open references to DOI-identified works present in Crossref, as of the latest COCI update. COCI does not index Crossref references that are not open, nor Crossref open references to entities that lack DOIs. The citations available in COCI are treated as first-class data entities, with accompanying properties including the citations timespan, modelled according to the OpenCitations Data Model.\n",
    "(This page is a legacy page (not linked anymore from the official website) that describes COCI. Since October 2023, all the citation data collected previously in different OpenCitations Indexes have been moved (and deduplicated) in the new citation collection, i.e. the OpenCitations Index.)\n",
    "\n",
    "- The [Crossref REST API][] is one of a variety of tools and APIs that allow anybody to search and reuse our members' metadata in sophisticated ways. If you read nothing else, please at least look at the API TIPs document and the \"Etiquette\" section of this document. It will save you (and us) much heartburn.\n",
    "\n",
    "- [OpenCitations Index][]: A citation index is a bibliographic index recording citations between publications, allowing the user to establish which later documents cite (i.e. contain references to) earlier documents. Several citation indexes are already available, some of which are freely accessible but not downloadable (e.g. Google Scholar), while others can be accessed only by paying significant subscription/access fees (e.g. Web of Science and Scopus).\n",
    "OpenCitations, as an infrastructure organization for open scholarship, has built the OpenCitations Index, a totally open citation index built on the bibliographic metadata for citing and cited publications that are stored in a separate database, OpenCitations Meta, using information available from a number of open bibliographic databases.\n",
    "\n",
    "[OpenCitations Meta database]: http://opencitations.net/meta     \"OpenCitations Meta\"\n",
    "[full dump]: http://opencitations.net/download#meta    \"full OpenCitations dump\"\n",
    "[Crossref REST API]: https://github.com/CrossRef/rest-api-doc#resource-components   \"Crossref REST API\"\n",
    "[COCI]: https://opencitations.net/index/coci   \"COCI\"\n",
    "[OpenCitations Index]: https://opencitations.net/index     \"OpenCitations Index\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
